{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "import glob\n",
    "from eqcorrscan.core.match_filter import Template\n",
    "import os\n",
    "from eqcorrscan import tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Template Creation\n",
    "\n",
    "# Get the path for the test-data so we can test this\n",
    "TEST_PATH = os.path.dirname(tests.__file__)\n",
    "sac_files = glob.glob(TEST_PATH + '/test_data/SAC/2014p611252/*')\n",
    "# sac_files is now a list of all the SAC files for event id:2014p611252\n",
    "template = Template().construct(\n",
    "    method='from_sac', name='test', lowcut=2.0, highcut=8.0,\n",
    "    samp_rate=20.0, filt_order=4, prepick=0.1, swin='all',\n",
    "    length=2.0, sac_files=sac_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Pick for CLV.DPZ has no phase hint given, you should not use this template for cross-correlation re-picking!\n",
      "Pick for BKS.HHZ has no phase hint given, you should not use this template for cross-correlation re-picking!\n",
      "Pick for GASB.HHZ has no phase hint given, you should not use this template for cross-correlation re-picking!\n",
      "Pick for HAST.HHZ has no phase hint given, you should not use this template for cross-correlation re-picking!\n",
      "Pick for HATC.HHZ has no phase hint given, you should not use this template for cross-correlation re-picking!\n"
     ]
    }
   ],
   "source": [
    "# Tribe Creation\n",
    "\n",
    "from eqcorrscan.core.match_filter import Tribe\n",
    "from obspy.clients.fdsn import Client\n",
    "\n",
    "client = Client('NCEDC')\n",
    "catalog = client.get_events(eventid='72572665', includearrivals=True)\n",
    "# To speed the example we have a catalog of one event, but you can have\n",
    "# more, we are also only using the first five picks, again to speed the\n",
    "# example.\n",
    "catalog[0].picks = catalog[0].picks[0:5]\n",
    "tribe = Tribe().construct(\n",
    "    method='from_client', catalog=catalog, client_id='NCEDC', lowcut=2.0,\n",
    "    highcut=8.0,  samp_rate=20.0, filt_order=4, length=6.0, prepick=0.1,\n",
    "    swin='all', process_len=3600, all_horiz=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Last bit of data between 2016-01-02T00:59:20.448391Z and 2016-01-02T01:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T01:59:20.448391Z and 2016-01-02T02:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T02:59:20.448391Z and 2016-01-02T03:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T03:59:20.448391Z and 2016-01-02T04:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T04:59:20.448391Z and 2016-01-02T05:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T05:59:20.448391Z and 2016-01-02T06:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T06:59:20.448391Z and 2016-01-02T07:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T07:59:20.448391Z and 2016-01-02T08:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T08:59:20.448391Z and 2016-01-02T09:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T09:59:20.448391Z and 2016-01-02T10:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T10:59:20.448391Z and 2016-01-02T11:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T11:59:20.448391Z and 2016-01-02T12:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T12:59:20.448391Z and 2016-01-02T13:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T13:59:20.448391Z and 2016-01-02T14:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T14:59:20.448391Z and 2016-01-02T15:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T15:59:20.448391Z and 2016-01-02T16:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T16:59:20.448391Z and 2016-01-02T17:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T17:59:20.448391Z and 2016-01-02T18:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T18:59:20.448391Z and 2016-01-02T19:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T19:59:20.448391Z and 2016-01-02T20:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T20:59:20.448391Z and 2016-01-02T21:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T21:59:20.448393Z and 2016-01-02T22:00:39.548393Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T22:59:20.448391Z and 2016-01-02T23:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n",
      "Last bit of data between 2016-01-02T23:59:20.448391Z and 2016-01-03T00:00:39.548391Z will go unused because it is shorter than a chunk of 3600.0 s\n",
      "plotvar is depreciated, use plot instead\n"
     ]
    }
   ],
   "source": [
    "# Match-filter detection using a Tribe\n",
    "\n",
    "from obspy import UTCDateTime\n",
    "party, stream = tribe.client_detect(\n",
    "    client=client, starttime=UTCDateTime(2016, 1, 2),\n",
    "    endtime=UTCDateTime(2016, 1, 3), threshold=8, threshold_type='MAD',\n",
    "    trig_int=6, plotvar=False, return_stream=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'read_detections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-221d1a4725a2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Generating a Party from a detection csv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread_detections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdetection_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mparty\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mParty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtemplate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtribe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'read_detections' is not defined"
     ]
    }
   ],
   "source": [
    "# Generating a Party from a detection csv\n",
    "\n",
    "detections = read_detections(detection_file) \n",
    "party = Party() \n",
    "for template in tribe: \n",
    "    template_detections = [d for d in detections\n",
    "                          if d.template_name == template.name]\n",
    "    family = Family(template=template, detections=template_detections)\n",
    "    party += family"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lag-calc using a Party\n",
    "\n",
    "stream = stream.merge().sort(['station'])\n",
    "repicked_catalog = party.lag_calc(stream, pre_processed=False,\n",
    "                                  shift_len=0.2, min_cc=0.4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple example - match-filter.match-filter\n",
    "\n",
    "from eqcorrscan.core.match_filter import match_filter\n",
    "from eqcorrscan.utils import pre_processing\n",
    "from obspy import read\n",
    "\n",
    "# Read in and process the daylong data\n",
    "st = read('continuous_data')\n",
    "# Use the same filtering and sampling parameters as your template!\n",
    "st = pre_processing.dayproc(\n",
    "    st, lowcut=2, highcut=10, filt_order=4, samp_rate=50,\n",
    "    starttime=st[0].stats.starttime.date)\n",
    "# Read in the templates\n",
    "templates = []\n",
    "template_names = ['template_1', 'template_2']\n",
    "for template_file in template_names:\n",
    "     templates.append(read(template_file))\n",
    "detections = match_filter(\n",
    "     template_names=template_names, template_list=templates, st=st,\n",
    "     threshold=8, threshold_type='MAD', trig_int=6, plotvar=False, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To write all detections to one file\n",
    "\n",
    "for detection in detections:\n",
    "     detection.write('my_first_detections.csv', append=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced example\n",
    "\n",
    "\"\"\"\n",
    "Simple tutorial to demonstrate some of the basic capabilities of the EQcorrscan\n",
    "matched-filter detection routine.  This builds on the template generation\n",
    "tutorial and uses those templates.  If you haven't run that tutorial script\n",
    "then you will need to before you can run this script.\n",
    "\"\"\"\n",
    "\n",
    "import glob\n",
    "import logging\n",
    "\n",
    "from http.client import IncompleteRead\n",
    "from multiprocessing import cpu_count\n",
    "from obspy.clients.fdsn import Client\n",
    "from obspy import UTCDateTime, Stream, read\n",
    "\n",
    "from eqcorrscan.utils import pre_processing\n",
    "from eqcorrscan.utils import plotting\n",
    "from eqcorrscan.core import match_filter\n",
    "\n",
    "# Set up logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s\\t%(name)s\\t%(levelname)s\\t%(message)s\")\n",
    "\n",
    "\n",
    "def run_tutorial(plot=False, process_len=3600, num_cores=cpu_count(),\n",
    "                 **kwargs):\n",
    "    \"\"\"Main function to run the tutorial dataset.\"\"\"\n",
    "    # First we want to load our templates\n",
    "    template_names = glob.glob('tutorial_template_*.ms')\n",
    "\n",
    "    if len(template_names) == 0:\n",
    "        raise IOError('Template files not found, have you run the template ' +\n",
    "                      'creation tutorial?')\n",
    "\n",
    "    templates = [read(template_name) for template_name in template_names]\n",
    "\n",
    "    # Work out what stations we have and get the data for them\n",
    "    stations = []\n",
    "    for template in templates:\n",
    "        for tr in template:\n",
    "            stations.append((tr.stats.station, tr.stats.channel))\n",
    "    # Get a unique list of stations\n",
    "    stations = list(set(stations))\n",
    "\n",
    "    # We will loop through the data chunks at a time, these chunks can be any\n",
    "    # size, in general we have used 1 day as our standard, but this can be\n",
    "    # as short as five minutes (for MAD thresholds) or shorter for other\n",
    "    # threshold metrics. However the chunk size should be the same as your\n",
    "    # template process_len.\n",
    "\n",
    "    # You should test different parameters!!!\n",
    "    start_time = UTCDateTime(2016, 1, 4)\n",
    "    end_time = UTCDateTime(2016, 1, 5)\n",
    "    chunks = []\n",
    "    chunk_start = start_time\n",
    "    while chunk_start < end_time:\n",
    "        chunk_end = chunk_start + process_len\n",
    "        if chunk_end > end_time:\n",
    "            chunk_end = end_time\n",
    "        chunks.append((chunk_start, chunk_end))\n",
    "        chunk_start += process_len\n",
    "\n",
    "    unique_detections = []\n",
    "\n",
    "    # Set up a client to access the GeoNet database\n",
    "    client = Client(\"GEONET\")\n",
    "\n",
    "    # Note that these chunks do not rely on each other, and could be paralleled\n",
    "    # on multiple nodes of a distributed cluster, see the SLURM tutorial for\n",
    "    # an example of this.\n",
    "    for t1, t2 in chunks:\n",
    "        # Generate the bulk information to query the GeoNet database\n",
    "        bulk_info = []\n",
    "        for station in stations:\n",
    "            bulk_info.append(('NZ', station[0], '*',\n",
    "                              station[1][0] + 'H' + station[1][-1], t1, t2))\n",
    "\n",
    "        # Note this will take a little while.\n",
    "        print('Downloading seismic data, this may take a while')\n",
    "        st = Stream()\n",
    "        for _bulk in bulk_info:\n",
    "            try:\n",
    "                st += client.get_waveforms(*_bulk)\n",
    "            except IncompleteRead:\n",
    "                print(f\"Could not download {_bulk}\")\n",
    "        # Merge the stream, it will be downloaded in chunks\n",
    "        st.merge()\n",
    "\n",
    "        # Pre-process the data to set frequency band and sampling rate\n",
    "        # Note that this is, and MUST BE the same as the parameters used for\n",
    "        # the template creation.\n",
    "        print('Processing the seismic data')\n",
    "        st = pre_processing.shortproc(\n",
    "            st, lowcut=2.0, highcut=9.0, filt_order=4, samp_rate=20.0,\n",
    "            num_cores=num_cores, starttime=t1, endtime=t2)\n",
    "        # Convert from list to stream\n",
    "        st = Stream(st)\n",
    "\n",
    "        # Now we can conduct the matched-filter detection\n",
    "        detections = match_filter.match_filter(\n",
    "            template_names=template_names, template_list=templates,\n",
    "            st=st, threshold=8.0, threshold_type='MAD', trig_int=6.0,\n",
    "            plotvar=plot, plotdir='.', cores=num_cores,\n",
    "            plot_format='png', **kwargs)\n",
    "\n",
    "        # Now lets try and work out how many unique events we have just to\n",
    "        # compare with the GeoNet catalog of 20 events on this day in this\n",
    "        # sequence\n",
    "        for master in detections:\n",
    "            keep = True\n",
    "            for slave in detections:\n",
    "                if not master == slave and abs(master.detect_time -\n",
    "                                               slave.detect_time) <= 1.0:\n",
    "                    # If the events are within 1s of each other then test which\n",
    "                    # was the 'best' match, strongest detection\n",
    "                    if not master.detect_val > slave.detect_val:\n",
    "                        keep = False\n",
    "                        print('Removed detection at %s with cccsum %s'\n",
    "                              % (master.detect_time, master.detect_val))\n",
    "                        print('Keeping detection at %s with cccsum %s'\n",
    "                              % (slave.detect_time, slave.detect_val))\n",
    "                        break\n",
    "            if keep:\n",
    "                unique_detections.append(master)\n",
    "                print('Detection at :' + str(master.detect_time) +\n",
    "                      ' for template ' + master.template_name +\n",
    "                      ' with a cross-correlation sum of: ' +\n",
    "                      str(master.detect_val))\n",
    "                # We can plot these too\n",
    "                if plot:\n",
    "                    stplot = st.copy()\n",
    "                    template = templates[template_names.index(\n",
    "                        master.template_name)]\n",
    "                    lags = sorted([tr.stats.starttime for tr in template])\n",
    "                    maxlag = lags[-1] - lags[0]\n",
    "                    stplot.trim(starttime=master.detect_time - 10,\n",
    "                                endtime=master.detect_time + maxlag + 10)\n",
    "                    plotting.detection_multiplot(\n",
    "                        stplot, template, [master.detect_time.datetime])\n",
    "    print('We made a total of ' + str(len(unique_detections)) + ' detections')\n",
    "    return unique_detections\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    run_tutorial()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SLURM example refer to docs 4.2.12."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eqcorrscan",
   "language": "python",
   "name": "eqcorrscan"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
